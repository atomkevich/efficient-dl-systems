# Week 5: Data-parallel training

* Lecture slides: [./lecture5.pdf](./lecture5.pdf)
* Video '22 (russian): [video1](https://disk.yandex.ru/i/79er4QA2euMdew), [video2](https://disk.yandex.ru/i/-TOM6-lZiJ1FQA)
* Seminar: [link](./seminar.ipynb)
* Homework: TBA

## Further reading
* [Python multiprocessing docs](https://docs.python.org/3/library/multiprocessing.html) (pay attention to `fork` vs `spawn`!)
* [PyTorch Distributed tutorial](https://pytorch.org/tutorials/intermediate/dist_tuto.html)
* [Collective communication protocols in NCCL](https://images.nvidia.com/events/sc15/pdfs/NCCL-Woolley.pdf)
* There's a ton of links on the slides, please check PDF.
